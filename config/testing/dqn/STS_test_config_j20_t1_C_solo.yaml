##############################################################################
###                               Testing                                  ###
##############################################################################

# (R)   [string]  RL algorithm you want to use - This template is for dqn
algorithm: dqn
# (R)   [string]  Path to the file with generated data that to be used for training
instances_file: fjssp/test_config_j20_t1_C_solo.pkl
# (R)   [string]  Test Environment: Should be the same the agent was trained on
                  # to stay consistent with action and observation spaces
environment: env_tetris_scheduling_indirect_action
# (O)   [str]     The reward strategy determines, how the reward is computed. Default is 'dense_makespan_reward'
reward_strategy: co2_makespan_tardiness
# (O)   [int]     The reward scale is a float by which the reward is multiplied to increase/decrease the reward signal
                  # strength
reward_scale: 1
# (O)   [string]  Name of the model you want to load for testing
saved_model_name: DQN_64-64_config_j20_t1_A_0-0-1_99
# (R)   [int]     Seed for all pseudo random generators (random, numpy, torch)
seed: 2
# (R)   [string]  Set an individual description that you can identify this
                  # training run  more easily later on. This will be used in
                  # "weights and biases" as well
config_description: DQN_64-64_config_j20_t1_A_0-0-1_99
# (O)   [string]  Set a directory from where you want to load the agent model
experiment_save_path: models
# (O)   [int]:    wandb mode choose: choose from [0: no wandb, 1: wandb_offline, 2: wandb_online]
wandb_mode: 2
# (O)   [string]  Set a wandb project where you want to upload all wandb logs
wandb_project: test_config_j20_t1_C_solo
# (R)   List[str] List of all heuristics and algorithms against which to benchmark
test_heuristics: []
# STS >>>
# this is a factor which is set for each parameter in the calculation for the reward
# 0 - parameter is not considered
reward_strategy_weight: {'co2': 1, 'makespan': 1, 'tardniess': 1}
# (O) List[List[int]] CO2e consumption per timestep, [[CO2e/kg, up_to_timestep]]
                  # After the last timestep, it will start with the first
                  # every timestep is 1 hour, day 1 = 0 to 23 timesteps, day 2 = 24 to 47, and so on
co2_timesteps:    [[5, 495.6], [8, 366.5], [12, 279.4],[18, 549.7],[24, 621.3],[29, 645.8],[33, 465.1],[37, 366.4],[42, 617.3],[48, 723.4]]
# reward is calculated with costs
co2_costs:        0.03 # €/kg
tardiness_costs:  5.0 # €/timestep
makespan_costs:   1.0 # €/gapstep
# <<< STS